<!DOCTYPE html>
<html lang="en">
<head><script src="/livereload.js?mindelay=10&amp;v=2&amp;port=1313&amp;path=livereload" data-no-instant defer></script>
  
    <title>How  to create a 3-node kubernetes cluster using kubeadm :: Terminal</title>
  
  <meta http-equiv="content-type" content="text/html; charset=utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<meta name="description" content="How to create a 3-node kubernetes cluster using kubeadm In kubernetes, a cluster is basicaly a collection of nodes. We majorly have a Control plane that does all the administrative operations and the Nodes whichs are responsible for housing the pods. Control plane This is the brains of the whole sytem, processes instrictions and sends them to the nodes via the kubelet The control plane contains the following: Apiserver Etcd kubelet kube-scheduler kubectl Apiserver This is the tool that receives requests from users and redirects it to different components within the controlplane It acts as a gateway to the outside world and receives requests from the user using the kubectl client If a request is sent to create a pod kubectl run nginx --image nginx, the request is sent to the kube-scheduler which comunicates with the kubelets to know which node is going to process the workload The kubelets are found in every node and is the gateway between the nodes and the control plane It keeps regular watch for any instructions coming from the apiserver which is sent as a podspec The kubelet checks the podspec to see if the pods are in the described state, if not so, it communictaes with the container runtime to crate pods so as to meet the desired state. The setup This is a basic setup running on a proxmox server. I have three VMs control, Node0 and Node1 the main intention is to create a kubentes cluster
" />
<meta name="keywords" content="k8s, kubead, pods, Nodes" />

  <meta name="robots" content="noodp" />

<link rel="canonical" href="http://localhost:1313/posts/how-to-create-a-kubenetes-cluster/" />





  
  <link rel="stylesheet" href="http://localhost:1313/css/buttons.min.2bc533403a27dfe0e93105a92502b42ce4587e2e4a87d9f7d349e51e16e09478.css">

  
  <link rel="stylesheet" href="http://localhost:1313/css/code.min.00125962708925857e7b66dbc58391d55be1191a3d0ce2034de8c9cd2c481c36.css">

  
  <link rel="stylesheet" href="http://localhost:1313/css/fonts.min.4881f0c525f3ce2a1864fb6e96676396cebe1e6fcef1933e8e1dde7041004fb5.css">

  
  <link rel="stylesheet" href="http://localhost:1313/css/footer.min.2e3eb191baee58dd05a9f0104ac1fab0827bca7c64dafe0b2579f934c33a1d69.css">

  
  <link rel="stylesheet" href="http://localhost:1313/css/gist.min.a751e8b0abe1ba8bc53ced52a38b19d8950fe78ca29454ea8c2595cf26aad5c0.css">

  
  <link rel="stylesheet" href="http://localhost:1313/css/header.min.b6fb4423cf82a9f9d7abc9cd010223fa3d70a6526a3f28f8e17d814c06e18f9e.css">

  
  <link rel="stylesheet" href="http://localhost:1313/css/main.min.fe8dc560fccb53a458b0db19ccb7b265764ac46b68596b7e099c6793054dd457.css">

  
  <link rel="stylesheet" href="http://localhost:1313/css/menu.min.83637a90d903026bc280d3f82f96ceb06c5fc72b7c1a8d686afb5bbf818a29f7.css">

  
  <link rel="stylesheet" href="http://localhost:1313/css/pagination.min.82f6400eae7c7c6dc3c866733c2ec0579e4089608fea69400ff85b3880aa0d3c.css">

  
  <link rel="stylesheet" href="http://localhost:1313/css/post.min.fc74ca360273c1d828da3c02b8174eba435607b369d98418ccc6f2243cd4e75d.css">

  
  <link rel="stylesheet" href="http://localhost:1313/css/prism.min.9023bbc24533d09e97a51a0a42a5a7bfe4c591ae167c5551fb1d2191d11977c0.css">

  
  <link rel="stylesheet" href="http://localhost:1313/css/syntax.min.cc789ed9377260d7949ea4c18781fc58959a89287210fe4edbff44ebfc1511b6.css">

  
  <link rel="stylesheet" href="http://localhost:1313/css/terminal.min.dd0bf9c7cacb24c1b0184f52f1869b274e06689557468cc7030ccf632328eb97.css">

  
  <link rel="stylesheet" href="http://localhost:1313/css/terms.min.b81791663c3790e738e571cdbf802312390d30e4b1d8dc9d814a5b5454d0ac11.css">







<link rel="shortcut icon" href="http://localhost:1313/favicon.png">
<link rel="apple-touch-icon" href="http://localhost:1313/apple-touch-icon.png">


<meta name="twitter:card" content="summary" />



<meta property="og:locale" content="en" />
<meta property="og:type" content="article" />
<meta property="og:title" content="How  to create a 3-node kubernetes cluster using kubeadm">
<meta property="og:description" content="How to create a 3-node kubernetes cluster using kubeadm In kubernetes, a cluster is basicaly a collection of nodes. We majorly have a Control plane that does all the administrative operations and the Nodes whichs are responsible for housing the pods. Control plane This is the brains of the whole sytem, processes instrictions and sends them to the nodes via the kubelet The control plane contains the following: Apiserver Etcd kubelet kube-scheduler kubectl Apiserver This is the tool that receives requests from users and redirects it to different components within the controlplane It acts as a gateway to the outside world and receives requests from the user using the kubectl client If a request is sent to create a pod kubectl run nginx --image nginx, the request is sent to the kube-scheduler which comunicates with the kubelets to know which node is going to process the workload The kubelets are found in every node and is the gateway between the nodes and the control plane It keeps regular watch for any instructions coming from the apiserver which is sent as a podspec The kubelet checks the podspec to see if the pods are in the described state, if not so, it communictaes with the container runtime to crate pods so as to meet the desired state. The setup This is a basic setup running on a proxmox server. I have three VMs control, Node0 and Node1 the main intention is to create a kubentes cluster
" />
<meta property="og:url" content="http://localhost:1313/posts/how-to-create-a-kubenetes-cluster/" />
<meta property="og:site_name" content="Terminal" />

  
  
  <meta property="og:image" content="http://localhost:1313/">

<meta property="og:image:width" content="1200">
<meta property="og:image:height" content="627">


  <meta property="article:published_time" content="2025-09-03 12:42:47 &#43;0300 EAT" />












</head>
<body>


<div class="container">

  <header class="header">
  <div class="header__inner">
    <div class="header__logo">
      <a href="/">
  <div class="logo">
    Alex Nduta
  </div>
</a>

    </div>
    
      <ul class="menu menu--mobile">
  <li class="menu__trigger">Menu&nbsp;▾</li>
  <li>
    <ul class="menu__dropdown">
      
        
          <li><a href="/about">About</a></li>
        
      
        
          <li><a href="/articles">Articles</a></li>
        
      
        
          <li><a href="/showcase">CV</a></li>
        
      
        
          <li><a href="/projects">Projects</a></li>
        
      
      
    </ul>
  </li>
</ul>

    
    
  </div>
  
    <nav class="navigation-menu">
  <ul class="navigation-menu__inner menu--desktop">
    
      
        
          <li><a href="/about" >About</a></li>
        
      
        
          <li><a href="/articles" >Articles</a></li>
        
      
        
          <li><a href="/showcase" >CV</a></li>
        
      
        
          <li><a href="/projects" >Projects</a></li>
        
      
      
    
  </ul>
</nav>

  
</header>


  <div class="content">
    
<article class="post">
  <h1 class="post-title">
    <a href="http://localhost:1313/posts/how-to-create-a-kubenetes-cluster/">How  to create a 3-node kubernetes cluster using kubeadm</a>
  </h1>
  <div class="post-meta"><time class="post-date">2025-09-03</time><span class="post-author">Alex Kinyanjui</span></div>

  
    <span class="post-tags">
      
      #<a href="http://localhost:1313/tags/k8s/">k8s</a>&nbsp;
      
      #<a href="http://localhost:1313/tags/kubernetes/">kubernetes</a>&nbsp;
      
      #<a href="http://localhost:1313/tags/kubeadm/">kubeadm</a>&nbsp;
      
    </span>
  
  


  

  <div class="post-content"><div>
        <h1 id="how--to-create-a-3-node-kubernetes-cluster-using-kubeadm">How  to create a 3-node kubernetes cluster using kubeadm<a href="#how--to-create-a-3-node-kubernetes-cluster-using-kubeadm" class="hanchor" ariaLabel="Anchor">#</a> </h1>
<ul>
<li>In kubernetes, a cluster is basicaly a collection of nodes. We majorly have a <code>Control plane</code> that does all the administrative operations and the <code>Nodes</code> whichs are responsible for housing the <code>pods</code>.</li>
</ul>
<h2 id="control-plane">Control plane<a href="#control-plane" class="hanchor" ariaLabel="Anchor">#</a> </h2>
<ul>
<li>This is the brains of the whole sytem, processes instrictions and sends them to the nodes via the <code>kubelet</code></li>
<li>The control plane contains the following:
<ul>
<li>Apiserver</li>
<li>Etcd</li>
<li>kubelet</li>
<li>kube-scheduler</li>
<li>kubectl</li>
</ul>
</li>
</ul>
<h3 id="apiserver">Apiserver<a href="#apiserver" class="hanchor" ariaLabel="Anchor">#</a> </h3>
<ul>
<li>This is the tool that receives requests from users and redirects it to different components within the controlplane</li>
<li>It acts as a gateway to the outside world and receives requests from the user using the <code>kubectl</code> client</li>
<li>If a request is sent to create a pod <code>kubectl run nginx --image nginx</code>, the request is sent to the <code>kube-scheduler</code> which comunicates with the <code>kubelets</code> to know which node is going to process the workload</li>
<li>The kubelets are found in every node and is the gateway between the nodes and the control plane</li>
<li>It keeps regular watch for any instructions coming from the apiserver which is sent as a <code>podspec</code></li>
<li>The kubelet checks the podspec to see if the pods are in the described state, if not so, it communictaes with the <code>container runtime</code> to crate pods so as to meet the desired state.</li>
</ul>
<h2 id="the-setup">The setup<a href="#the-setup" class="hanchor" ariaLabel="Anchor">#</a> </h2>
<p>This is a basic setup running on a proxmox server. I have three VMs <code>control</code>, <code>Node0</code> and <code>Node1</code>
the main intention is to create a kubentes cluster</p>
<h2 id="the-process">The process<a href="#the-process" class="hanchor" ariaLabel="Anchor">#</a> </h2>
<ul>
<li>There are four phases to this process:
<ul>
<li><strong>prerequisites</strong>: preparing all of the three vms</li>
<li><strong>Control plane setup</strong>: initialising the cluster brain on the <code>control</code> node</li>
<li><strong>worker node setup</strong>: <code>nodde0</code> and <code>node1</code> joing the cluster</li>
<li><strong>Network Setup</strong>: Installing a network pluging so that the pods in the cluster can communicate</li>
</ul>
</li>
</ul>
<h2 id="1-prerequisites-this-is-to-be-done-on-all-three-nodes">1. Prerequisites: This is to be done on all three nodes<a href="#1-prerequisites-this-is-to-be-done-on-all-three-nodes" class="hanchor" ariaLabel="Anchor">#</a> </h2>
<ol>
<li><strong>Intsall a container runtime</strong></li>
</ol>
<ul>
<li>k8s never runs containes by itself, it depends on a container runtie to do this. K8s communictates with a container runtime via the  Container Runtime Interface<code>CRI</code>. This gives us the freedom to work with whichever runtime we want as long as it is compatible with the CRI.</li>
<li>examples of <a href="https://landscapeapp.cncf.io/cncf/card-mode?category=container-runtime&amp;grouping=category">container rutimes supported by kubentes include</a> include containerd, cri-o,  firecaker, kata</li>
<li>Containerd is the industry standard container runtime that k8s uses to manage containers</li>
</ul>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-sh" data-lang="sh"><span style="display:flex;"><span>$ sudo apt-get update
</span></span><span style="display:flex;"><span><span style="color:#75715e"># install containerd</span>
</span></span><span style="display:flex;"><span>$ sudo apt-get install -y containerd
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># Create the default configuration file</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># sudo mkdir -p /etc/containerd</span>
</span></span><span style="display:flex;"><span>$ containerd config default | sudo tee /etc/containerd/config.toml
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># configure containerd to use the system cgroup driver which helps in proper resource management</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># this command basically just edits the file `config.toml` and converts the value to `true`</span>
</span></span><span style="display:flex;"><span>$ sudo set -i <span style="color:#e6db74">&#39;s/SystemdCgroup = false/SystemdCgroup = true/&#39;</span> /etc/contained/config.toml
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># restart the containerd service</span>
</span></span><span style="display:flex;"><span>$ sudo systemctl restart containerd
</span></span></code></pre></div><ol start="2">
<li><strong>Disable swapp memory</strong></li>
</ol>
<ul>
<li>swap gives the system an illusion that there is more memory than the system has which could lead to underperfomance in context of kubernetes.</li>
<li>We are required to disable swapp as the <code>kubenetes scheduler</code> needs a predictable and stable measure of memory on every node.</li>
<li>By design, the <code>kubelet</code> fails if the swapp is turned on on a node</li>
</ul>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-sh" data-lang="sh"><span style="display:flex;"><span><span style="color:#75715e"># disable swap for the current session</span>
</span></span><span style="display:flex;"><span>$ sudo swapoff -a
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># permanetly disable swap by commentin the line swap in the fstab file</span>
</span></span><span style="display:flex;"><span>$ sudo set -i <span style="color:#e6db74">&#39;/ swap / s/^\(.*\)$/#\1/g&#39;</span> /etc/fstab
</span></span></code></pre></div><ol start="3">
<li><strong>configure critical kernel parameters</strong></li>
</ol>
<ul>
<li>we need to tell the linux kernel to allow IP traffic to be forwarded and let the <code>ip tables</code> to see the bridged network traffic.</li>
<li>k8s CNI plugins create a virtual network and the <code>bridge</code> is like a virtual swicth that every node gets to plugin to enable the communication between pods on diffrent nodes</li>
</ul>
<p><em>loading kernel modules</em></p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-sh" data-lang="sh"><span style="display:flex;"><span><span style="color:#75715e"># create a config file for these settings</span>
</span></span><span style="display:flex;"><span>cat <span style="color:#e6db74">&lt;&lt; EOF | sudo tee /etc/modules-load.d/k8s.conf
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">overlay
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">br_netfilter
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">EOF</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># load these modules into the kernel</span>
</span></span><span style="display:flex;"><span>sudo modprobe overlay
</span></span><span style="display:flex;"><span>sudo modeprobe br_netfilter
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># confirm if the modules has been loaded succesfully</span>
</span></span><span style="display:flex;"><span>$ lsmod | grep overlay
</span></span><span style="display:flex;"><span>$ lsmod | grep br_netfilter
</span></span></code></pre></div><ul>
<li>
<p><code>overlay</code> module is an ideal docker overlay filesytem which kubernetes uses to manage container layer. without it docker cannot funtion properly and kubernetes cannot ochestrate  containers</p>
</li>
<li>
<p><code>br_netfilter module</code>: this module is crucial for networking in k8s. It enables briding and filtering capabilities allowing k8s to manage network traffic between pods and services effectively</p>
<p><em>configuring systcl parameters</em></p>
</li>
</ul>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-sh" data-lang="sh"><span style="display:flex;"><span><span style="color:#75715e"># These paramaters are required for CNI plugins to work</span>
</span></span><span style="display:flex;"><span>$ cat <span style="color:#e6db74">&lt;&lt;EOF | sudo tee /etc/sysctl.d/k8s.conf
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">net.bridge.bridge-nf-call-iptables  = 1
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">net.bridge.bridge-nf-call-ip6tables = 1
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">net.ipv4.ip_forward                 = 1
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">EOF</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># apply the parameters without having to reboot</span>
</span></span><span style="display:flex;"><span>$ sudo sysctl --system
</span></span></code></pre></div><p><strong>4. Install <code>kubeadm</code>, <code>kubelet</code>  and <code>kubectl</code></strong></p>
<ul>
<li><code>kubeadm</code> use to manage cluster operations with commands such as <code>init</code> and <code>join</code></li>
<li><code>kubelet</code> This is the agent that runs on every node and listents to instructions from the control plane and manages the pods inside the nodes</li>
<li><code>kubectl</code> The commandline tool to interact with the cluster</li>
</ul>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-sh" data-lang="sh"><span style="display:flex;"><span><span style="color:#75715e"># update the package index and install packages required to install kubernetes apt repository</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>$ sudo apt-get update
</span></span><span style="display:flex;"><span>$ sudo apt-get install -y apt-transport-https ca-certificates curl gpg
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># Download the public siging key for the k8s package repository</span>
</span></span><span style="display:flex;"><span>$ curl -fsSL https://pkgs.k8s.io/core:/stable:/v1.34.0/deb/Release.key | sudo gpg --dearmor -o /etc/apt/keyrings/kubernetes-apt-keyring.gpg
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># Add the approprite kubernets apt repository</span>
</span></span><span style="display:flex;"><span>$ echo <span style="color:#e6db74">&#39;deb [signed-by=/etc/apt/keyrings/kubernetes-apt-keyring.gpg] https://pkgs.k8s.io/core:/stable:/v1.30/deb/ /&#39;</span> | sudo tee /etc/apt/sources.list.d/kubernetes.list
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># update packages and install kubelet, kubead and kubectl</span>
</span></span><span style="display:flex;"><span>$ sudo apt-get update
</span></span><span style="display:flex;"><span>$ sudo apt-get install -y kubeadm kubelet kubectl
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># pin the package version to prevent accidental upgrade</span>
</span></span><span style="display:flex;"><span>$ sudo apt-mark hold kubelet kubeadm kubectl
</span></span></code></pre></div><h2 id="2-initialise-the-control-plane--this-is-done-on-the-control-plane-alone">2. Initialise the control plane &gt; this is done on the control plane alone<a href="#2-initialise-the-control-plane--this-is-done-on-the-control-plane-alone" class="hanchor" ariaLabel="Anchor">#</a> </h2>
<ul>
<li>
<p>Considering that we have multiple nodes, w need to choose the node that will operate as the <code>control plane</code></p>
</li>
<li>
<p>Running <code>kubeadm init</code> bootstraps the control plane and setsup:</p>
<ul>
<li>etcd -&gt; the cluster db</li>
<li>ApI server</li>
<li>scheduler</li>
<li>and control manger</li>
</ul>
</li>
<li>
<p>we use the flag <code>--pod-network-cidr=192.168.0.0/16</code>   to prelocate IP address range for our pod network</p>
</li>
<li>
<p>The ip address range should not conflict with the VM Ip address</p>
</li>
</ul>
<pre tabindex="0"><code>$ sudo kubeadm init --pod-network-cidr=192.168.0.0/16
</code></pre><ul>
<li>The command might take a while, when done, it will print:
<ul>
<li>command to configure kubectl for your user</li>
<li><code>kubeadm join</code> hash with a unique token and hash</li>
</ul>
</li>
<li>you are required to copy the kubeadm join command as you will use it in your worker nodes</li>
</ul>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-sh" data-lang="sh"><span style="display:flex;"><span><span style="color:#75715e"># This is given as a response, use to to configure kubectl</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>mkdir -p $HOME/.kube
</span></span><span style="display:flex;"><span>sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
</span></span><span style="display:flex;"><span>sudo chown <span style="color:#66d9ef">$(</span>id -u<span style="color:#66d9ef">)</span>:<span style="color:#66d9ef">$(</span>id -g<span style="color:#66d9ef">)</span> $HOME/.kube/config
</span></span></code></pre></div><h2 id="3-join-the-worker-nodes--run-on-the-worker-nodes-only">3. Join the worker nodes &gt; run on the worker nodes only<a href="#3-join-the-worker-nodes--run-on-the-worker-nodes-only" class="hanchor" ariaLabel="Anchor">#</a> </h2>
<ul>
<li>This is how we tell our worker nodes how and where to fine the control plane and authenticate with the control API server</li>
<li>copy the <code>kubead join</code> command provided by the control plane</li>
</ul>
<h2 id="4-install-the-pod-network--run-on-the-control-node-only">4. Install the pod network &gt; run on the control node only<a href="#4-install-the-pod-network--run-on-the-control-node-only" class="hanchor" ariaLabel="Anchor">#</a> </h2>
<ul>
<li>At this point, the worker nodes have joined the cluster and running the command <code>kubectl get nodes</code> will return a list on woker nodes in <code>Ready</code> state.</li>
<li>The nodes are will registerd bu there is no network fabric connecting them. This means that pods on <code>node0</code> cannot talk to pods on <code>node 1</code></li>
<li>We need to install a Container Network Interface to create this virtual network.</li>
<li>We will use Callico</li>
</ul>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-sh" data-lang="sh"><span style="display:flex;"><span>kubectl apply -f https://raw.githubusercontent.com/projectcalico/calico/v3.28.0/manifests/calico.yaml
</span></span></code></pre></div><ul>
<li>After this, running the command <code>kubectl get nodes</code> will return nodes in the ready state</li>
</ul>
<p><strong>Reaources</strong>
<a href="https://kubernetes.io/docs/setup/production-environment/tools/kubeadm/create-cluster-kubeadm/">creating a cluster with kubeadm</a></p>

      </div></div>

  
    
<div class="pagination">
  <div class="pagination__title">
    <span class="pagination__title-h"></span>
    <hr />
  </div>
  <div class="pagination__buttons">
    
    
    
      <a href="http://localhost:1313/posts/how-to-upgrade-a-kubernetes-node-in-a-cluster/" class="button inline next">
         How to upgrade a kubernetes node in a cluster
      </a>
    
  </div>
</div>


  

  
    

  
</article>

  </div>

  
    <footer class="footer">
  <div class="footer__inner">
    
      <div class="copyright">
        <span>© 2025 Powered by <a href="https://gohugo.io">Hugo</a></span>
    
      <span>:: <a href="https://github.com/panr/hugo-theme-terminal" target="_blank">Theme</a> made by <a href="https://github.com/panr" target="_blank">panr</a></span>
      </div>
  </div>
</footer>






<script type="text/javascript" src="/bundle.min.js"></script>





  
</div>

</body>
</html>
